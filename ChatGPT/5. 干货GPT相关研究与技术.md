18篇文章。这部分内容主要面向研究人员与开发者，整理了机器之心报道的、来自各个机构的有关GPT的论文、技术解读、开源项目等内容，从中你可以学习到GPT的能力是如何一步步强化的。
- 特斯拉前A|总监教你手搓GPT大模型，教学视频已出
- 将GPT家族模型极限压缩，1700+亿参数稀疏性达50%性能不减，单GPU即可
- 速揽2500星，Andrej Karpathy重写了一份minGPT库
- ChatGPT背后的开源AI框架Ray, 现在值10亿美元
- 够快！爆火的ChatGPT等价开源项目来了，网友：我担心跑不起来
- 无需人工标注，自生成指令框架打破ChatGPT等LLM的成本瓶颈
- 被GPT带飞的ln-Context Learning为什么起作用？模型在秘密执行梯度下降
- GPT-3、Stable Diffusion一起助攻，让模型听懂甲方修图需求
- AI在线求鼓励？这些人一句话让GPT-3算术准确率提升61%
- 单个GPU无法训练GPT-3,但有了这个，你能调优超参数了
- 200亿参数GPT-NeoX即将开源：96块A100训练三个月，野生GPT家族再添一员
- 谁说GPT只擅长生成？清华、智源等研究力证：GPT语言理解能力不输BERT
- 27亿参数的「野生版」GPT-3开源，GitHub项目2.9K Star量
- GPT-3最新测试出炉：57项任务表现均低于专家水平，最不擅长STEM
- 一天star量破千，300行代码，特斯拉Al总监Karpathy写了个GPT的Pytorch训练库
- 完全图解GPT-2:看完这篇就够了（二）
- 完全图解GPT-2:看完这篇就够了（一）
- 有钱任性：英伟达训练80亿参数量GPT-2,
- 1475块V10053分钟训练BERT